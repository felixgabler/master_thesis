{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Employ transfer learning with new LMs for IDR prediction\n",
    "Dataset from [Disprot](https://www.disprot.org/download) (actually [older version with annotation](https://idpcentral.org/caid/data/1/reference/disprot-disorder.txt)). Methods used from [ProtTrans](https://github.com/agemagician/ProtTrans).\n",
    "\n",
    "Based on [PytorchLightning implementation](https://github.com/agemagician/ProtTrans/blob/master/Fine-Tuning/ProtBert-BFD-FineTuning-PyTorchLightning-MS.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon May 16 19:07:00 2022       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 470.57.02    Driver Version: 470.57.02    CUDA Version: 11.4     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:0A:00.0 Off |                  N/A |\r\n",
      "| 33%   49C    P8    11W / 250W |      6MiB / 11178MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  NVIDIA GeForce ...  On   | 00000000:0B:00.0 Off |                  N/A |\r\n",
      "| 27%   49C    P8    12W / 250W |      6MiB / 11178MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   2  NVIDIA GeForce ...  On   | 00000000:41:00.0 Off |                  N/A |\r\n",
      "| 25%   42C    P8    12W / 250W |      6MiB / 11178MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   3  NVIDIA GeForce ...  On   | 00000000:42:00.0 Off |                  N/A |\r\n",
      "| 25%   38C    P8    11W / 250W |     16MiB / 11176MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A      2305      G   /usr/lib/xorg/Xorg                  4MiB |\r\n",
      "|    1   N/A  N/A      2305      G   /usr/lib/xorg/Xorg                  4MiB |\r\n",
      "|    2   N/A  N/A      2305      G   /usr/lib/xorg/Xorg                  4MiB |\r\n",
      "|    3   N/A  N/A      2305      G   /usr/lib/xorg/Xorg                  9MiB |\r\n",
      "|    3   N/A  N/A      2481      G   /usr/bin/gnome-shell                3MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence, pad_sequence\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "from transformers import T5EncoderModel, T5Tokenizer\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from transformers import XLNetModel, XLNetTokenizer\n",
    "from transformers import AlbertModel, AlbertTokenizer\n",
    "\n",
    "from torchnlp.encoders import LabelEncoder\n",
    "from torchnlp.datasets.dataset import Dataset\n",
    "from torchnlp.utils import collate_tensors\n",
    "\n",
    "from test_tube import HyperOptArgumentParser\n",
    "import os\n",
    "import re\n",
    "import gc\n",
    "from datetime import datetime\n",
    "import logging as log\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "4"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Select the model\n",
    "model_name = \"Rostlab/prot_t5_xl_uniref50\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class DisorderDataset:\n",
    "    \"\"\"\n",
    "    Loads the Dataset from the txt files passed to the parser.\n",
    "    \"\"\"\n",
    "\n",
    "    def collate_lists(self, seqs: list, labels: list) -> list[dict]:\n",
    "        \"\"\" Converts each line into a dictionary. \"\"\"\n",
    "        collated_dataset = []\n",
    "        for i in range(len(seqs)):\n",
    "            collated_dataset.append({\"seq\": seqs[i], \"label\": labels[i]})\n",
    "        return collated_dataset\n",
    "\n",
    "    def load_dataset(self, path):\n",
    "        seqs = []\n",
    "        labels = []\n",
    "        with open(path) as file_handler:\n",
    "            i = -1\n",
    "            for line in file_handler:\n",
    "                i += 1\n",
    "                if i < 10 or len(line) > 1536:\n",
    "                    continue\n",
    "                i_offset = i - 10\n",
    "                if i_offset % 7 == 1:\n",
    "                    # Map rare amino acids\n",
    "                    seqs.append(\" \".join(list(re.sub(r\"[UZOB]\", \"X\", line.strip()))))\n",
    "                elif i_offset % 7 == 2:\n",
    "                    labels.append(line.strip())\n",
    "\n",
    "        assert len(seqs) == len(labels)\n",
    "        return Dataset(self.collate_lists(seqs, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class ProtTransDisorderPredictor(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    ProtTrans model to predict intrinsical disorder in sequences.\n",
    "\n",
    "    :param hp: ArgumentParser containing the hyperparameters.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hp) -> None:\n",
    "        super(ProtTransDisorderPredictor, self).__init__()\n",
    "        self.hp = hp\n",
    "        self.batch_size = self.hp.batch_size\n",
    "\n",
    "        self.model_name = model_name\n",
    "\n",
    "        self.dataset = DisorderDataset()\n",
    "\n",
    "        self.metric_acc = Accuracy(ignore_index=-100)\n",
    "\n",
    "        # build model\n",
    "        self.__build_model()\n",
    "\n",
    "        # Loss criterion initialization.\n",
    "        self.__build_loss()\n",
    "\n",
    "        if self.hp.nr_frozen_epochs > 0:\n",
    "            self.freeze_encoder()\n",
    "        else:\n",
    "            self._frozen = False\n",
    "        self.nr_frozen_epochs = self.hp.nr_frozen_epochs\n",
    "\n",
    "    def __build_model(self) -> None:\n",
    "        \"\"\" Init BERT model + tokenizer + classification head.\"\"\"\n",
    "        if \"t5\" in self.model_name:\n",
    "            self.tokenizer = T5Tokenizer.from_pretrained(self.model_name, do_lower_case=False)\n",
    "            self.LM = T5EncoderModel.from_pretrained(self.model_name)\n",
    "        elif \"albert\" in self.model_name:\n",
    "            self.tokenizer = AlbertTokenizer.from_pretrained(self.model_name, do_lower_case=False)\n",
    "            self.LM = AlbertModel.from_pretrained(self.model_name)\n",
    "        elif \"bert\" in self.model_name:\n",
    "            self.tokenizer = BertTokenizer.from_pretrained(self.model_name, do_lower_case=False)\n",
    "            self.LM = BertModel.from_pretrained(self.model_name)\n",
    "        elif \"xlnet\" in self.model_name:\n",
    "            self.tokenizer = XLNetTokenizer.from_pretrained(self.model_name, do_lower_case=False)\n",
    "            self.LM = XLNetModel.from_pretrained(self.model_name)\n",
    "        else:\n",
    "            print(\"Unkown model name\")\n",
    "\n",
    "        if self.hp.gradient_checkpointing:\n",
    "            self.LM.gradient_checkpointing_enable()\n",
    "\n",
    "        # Label Encoder\n",
    "        self.label_encoder = LabelEncoder(self.hp.label_set.split(\",\"), reserved_labels=[], unknown_index=None)\n",
    "        self.hidden_features = 1024\n",
    "\n",
    "        # https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html\n",
    "        # https://www.dotlayer.org/en/training-rnn-using-pytorch/\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.LM.config.hidden_size,\n",
    "            hidden_size=self.hidden_features,\n",
    "            num_layers=1,\n",
    "            bidirectional=self.hp.bidirectional_lstm,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        self.hidden2label = nn.Linear(2 * self.hidden_features if self.hp.bidirectional_lstm else self.hidden_features,\n",
    "                                      self.label_encoder.vocab_size)\n",
    "\n",
    "    def __build_loss(self):\n",
    "        \"\"\" Initializes the loss function/s. \"\"\"\n",
    "        self._loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def unfreeze_encoder(self) -> None:\n",
    "        \"\"\" un-freezes the encoder layer. \"\"\"\n",
    "        if self._frozen:\n",
    "            log.info(f\"\\n-- Encoder model fine-tuning\")\n",
    "            for param in self.LM.parameters():\n",
    "                param.requires_grad = True\n",
    "            self._frozen = False\n",
    "\n",
    "    def freeze_encoder(self) -> None:\n",
    "        \"\"\" freezes the encoder layer. \"\"\"\n",
    "        for param in self.LM.parameters():\n",
    "            param.requires_grad = False\n",
    "        self._frozen = True\n",
    "\n",
    "    def predict(self, sample: dict) -> dict:\n",
    "        \"\"\" Predict function.\n",
    "        :param sample: dictionary with the text we want to classify.\n",
    "        Returns:\n",
    "            Dictionary with the input text and the predicted label.\n",
    "        \"\"\"\n",
    "        if self.training:\n",
    "            self.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model_input, _ = self.prepare_sample([sample], prepare_target=False)\n",
    "            model_out = self.forward(**model_input)\n",
    "            predicted_labels = [\n",
    "                self.label_encoder.index_to_token[prediction]\n",
    "                for prediction in model_out\n",
    "            ]\n",
    "            sample[\"predicted_label\"] = predicted_labels[0]\n",
    "\n",
    "        return sample\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, length):\n",
    "        \"\"\" Usual pytorch forward function.\n",
    "        input ids is already padded\n",
    "        Returns:\n",
    "            model outputs\n",
    "        \"\"\"\n",
    "        input_ids = torch.tensor(input_ids, device=self.device)\n",
    "        attention_mask = torch.tensor(attention_mask, device=self.device)\n",
    "\n",
    "        padded_word_embeddings = self.LM(input_ids, attention_mask)[0]\n",
    "\n",
    "        # We pack the padded sequence to improve the computational speed during training\n",
    "        pack_padded_sequences_vectors = pack_padded_sequence(padded_word_embeddings, length, batch_first=True)\n",
    "\n",
    "        lstm_out, _ = self.lstm(pack_padded_sequences_vectors)\n",
    "        lstm_out, _ = pad_packed_sequence(lstm_out, batch_first=True, total_length=self.hp.max_length)\n",
    "\n",
    "        tag_space = self.hidden2label(lstm_out)\n",
    "\n",
    "        tag_scores = F.softmax(tag_space, dim=1)\n",
    "\n",
    "        return tag_scores.transpose(-1, 1)\n",
    "\n",
    "    def loss(self, predictions: torch.tensor, targets: torch.tensor) -> torch.tensor:\n",
    "        \"\"\"\n",
    "        Computes Loss value according to a loss function.\n",
    "        :param predictions: a tensor [batch_size x 1] with model predictions\n",
    "        :param targets: Label values [batch_size]\n",
    "        Returns:\n",
    "            torch.tensor with loss value.\n",
    "        \"\"\"\n",
    "        return self._loss(predictions, targets)\n",
    "\n",
    "    def prepare_sample(self, sample: list, prepare_target: bool = True) -> (dict, dict):\n",
    "        \"\"\"\n",
    "        Function that prepares a sample to input the model.\n",
    "        :param prepare_target: also load label\n",
    "        :param sample: list of dictionaries.\n",
    "\n",
    "        Returns:\n",
    "            - dictionary with the expected model inputs.\n",
    "            - dictionary with the expected target labels.\n",
    "        \"\"\"\n",
    "        sample = collate_tensors(sample)\n",
    "\n",
    "        inputs = self.tokenizer.batch_encode_plus(sample[\"seq\"],\n",
    "                                                  add_special_tokens=True,\n",
    "                                                  padding='max_length',\n",
    "                                                  #is_split_into_words=True,\n",
    "                                                  return_length=True,\n",
    "                                                  truncation=True,\n",
    "                                                  max_length=self.hp.max_length)\n",
    "\n",
    "        if not prepare_target:\n",
    "            return inputs, {}\n",
    "\n",
    "        # Prepare target:\n",
    "        try:\n",
    "            labels = [self.label_encoder.batch_encode(l) for l in sample[\"label\"]]\n",
    "            labels.append(torch.empty(self.hp.max_length))\n",
    "            padded_sequences_labels = pad_sequence(labels, batch_first=True, padding_value=-100)\n",
    "            return inputs, padded_sequences_labels[:-1]\n",
    "        except RuntimeError:\n",
    "            print(sample[\"label\"])\n",
    "            raise Exception(\"Label encoder found an unknown label.\")\n",
    "\n",
    "    def training_step(self, batch: tuple, batch_nb: int, *args, **kwargs) -> dict:\n",
    "        \"\"\"\n",
    "        Runs one training step. This usually consists in the forward function followed\n",
    "            by the loss function.\n",
    "\n",
    "        :param batch: The output of your dataloader.\n",
    "        :param batch_nb: Integer displaying which batch this is\n",
    "        Returns:\n",
    "            - dictionary containing the loss and the metrics to be added to the lightning logger.\n",
    "        \"\"\"\n",
    "        inputs, targets = batch\n",
    "        model_out = self.forward(**inputs)\n",
    "        loss = self.loss(model_out, targets)\n",
    "\n",
    "        return {'loss': loss}\n",
    "\n",
    "    def validation_step(self, batch: tuple, batch_nb: int, *args, **kwargs):\n",
    "        \"\"\" Similar to the training step but with the model in eval mode.\n",
    "        Returns:\n",
    "            - dictionary passed to the validation_end function.\n",
    "        \"\"\"\n",
    "        inputs, y = batch\n",
    "        model_out = self.forward(**inputs)\n",
    "        y_hat = torch.argmax(model_out, dim=1)\n",
    "\n",
    "        self.log('val_loss', self.loss(model_out, y))\n",
    "        self.log('val_acc', self.metric_acc(y_hat, y))\n",
    "\n",
    "    def test_step(self, batch: tuple, batch_nb: int, *args, **kwargs):\n",
    "        \"\"\" Similar to the training step but with the model in eval mode.\n",
    "        Returns:\n",
    "            - dictionary passed to the validation_end function.\n",
    "        \"\"\"\n",
    "        inputs, y = batch\n",
    "        model_out = self.forward(**inputs)\n",
    "        y_hat = torch.argmax(model_out, dim=1)\n",
    "\n",
    "        self.log('test_loss', self.loss(model_out, y))\n",
    "        self.log('test_acc', self.metric_acc(y_hat, y))\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\" Sets different Learning rates for different parameter groups. \"\"\"\n",
    "        parameters = [\n",
    "            {\"params\": self.hidden2label.parameters()},\n",
    "            {\"params\": self.lstm.parameters()},\n",
    "            {\n",
    "                \"params\": self.LM.parameters(),\n",
    "                \"lr\": self.hp.encoder_learning_rate,\n",
    "            },\n",
    "        ]\n",
    "        optimizer = optim.Adam(parameters, lr=self.hp.learning_rate)\n",
    "        return optimizer\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        \"\"\" Pytorch lightning hook \"\"\"\n",
    "        if self.current_epoch + 1 >= self.nr_frozen_epochs:\n",
    "            self.unfreeze_encoder()\n",
    "\n",
    "    def __retrieve_dataset(self, train=False, val=False, test=False):\n",
    "        \"\"\" Retrieves task specific dataset \"\"\"\n",
    "        if train:\n",
    "            return self.dataset.load_dataset(self.hp.train_file)\n",
    "        elif val:\n",
    "            return self.dataset.load_dataset(self.hp.val_file)\n",
    "        elif test:\n",
    "            return self.dataset.load_dataset(self.hp.test_file)\n",
    "        else:\n",
    "            print('Incorrect dataset split')\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        \"\"\" Function that loads the train set. \"\"\"\n",
    "        _train_dataset = self.__retrieve_dataset(train=True)\n",
    "        return DataLoader(\n",
    "            dataset=_train_dataset,\n",
    "            sampler=RandomSampler(_train_dataset),\n",
    "            batch_size=self.hp.batch_size,\n",
    "            collate_fn=self.prepare_sample,\n",
    "            num_workers=self.hp.loader_workers,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        \"\"\" Function that loads the validation set. \"\"\"\n",
    "        _dev_dataset = self.__retrieve_dataset(val=True)\n",
    "        return DataLoader(\n",
    "            dataset=_dev_dataset,\n",
    "            batch_size=self.hp.batch_size,\n",
    "            collate_fn=self.prepare_sample,\n",
    "            num_workers=self.hp.loader_workers,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self) -> DataLoader:\n",
    "        \"\"\" Function that loads the validation set. \"\"\"\n",
    "        _test_dataset = self.__retrieve_dataset(test=True)\n",
    "        return DataLoader(\n",
    "            dataset=_test_dataset,\n",
    "            batch_size=self.hp.batch_size,\n",
    "            collate_fn=self.prepare_sample,\n",
    "            num_workers=self.hp.loader_workers,\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def add_model_specific_args(\n",
    "        cls, parser: HyperOptArgumentParser\n",
    "    ) -> HyperOptArgumentParser:\n",
    "        \"\"\" Parser for Estimator specific arguments/hyperparameters.\n",
    "        :param parser: HyperOptArgumentParser obj\n",
    "        Returns:\n",
    "            - updated parser\n",
    "        \"\"\"\n",
    "        parser.opt_list(\n",
    "            \"--max_length\",\n",
    "            default=1536,\n",
    "            type=int,\n",
    "            help=\"Maximum sequence length.\",\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            \"--encoder_learning_rate\",\n",
    "            default=5e-06,\n",
    "            type=float,\n",
    "            help=\"Encoder specific learning rate.\",\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            \"--learning_rate\",\n",
    "            default=3e-05,\n",
    "            type=float,\n",
    "            help=\"Classification head learning rate.\",\n",
    "        )\n",
    "        parser.opt_list(\n",
    "            \"--nr_frozen_epochs\",\n",
    "            default=1,\n",
    "            type=int,\n",
    "            help=\"Number of epochs we want to keep the encoder model frozen.\",\n",
    "            tunable=True,\n",
    "            options=[0, 1, 2, 3, 4, 5],\n",
    "        )\n",
    "        # Data Args:\n",
    "        parser.add_argument(\n",
    "            \"--label_set\",\n",
    "            default=\"0,1\",\n",
    "            type=str,\n",
    "            help=\"Classification labels set.\",\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            \"--train_file\",\n",
    "            default=\"../data/disprot/flDPnn_Training_Annotation.txt\",\n",
    "            type=str,\n",
    "            help=\"Path to the file containing the train data.\",\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            \"--val_file\",\n",
    "            default=\"../data/disprot/flDPnn_Validation_Annotation.txt\",\n",
    "            type=str,\n",
    "            help=\"Path to the file containing the validation data.\",\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            \"--test_file\",\n",
    "            default=\"../data/disprot/flDPnn_Test_Annotation.txt\",\n",
    "            type=str,\n",
    "            help=\"Path to the file containing the test data.\",\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            \"--loader_workers\",\n",
    "            default=8,\n",
    "            type=int,\n",
    "            help=\"How many subprocesses to use for data loading. 0 means that \\\n",
    "                the data will be loaded in the main process.\",\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            \"--gradient_checkpointing\",\n",
    "            default=True,\n",
    "            type=bool,\n",
    "            help=\"Enable or disable gradient checkpointing which use the cpu memory \\\n",
    "                with the gpu memory to store the model.\",\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            \"--bidirectional_lstm\",\n",
    "            default=True,\n",
    "            type=bool,\n",
    "            help=\"Enable bidirectional LSTM in the decoder.\",\n",
    "        )\n",
    "        return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "parser = HyperOptArgumentParser(\n",
    "    strategy=\"random_search\",\n",
    "    description=\"ProtTrans IDR Predictor\",\n",
    "    add_help=True,\n",
    ")\n",
    "parser.add_argument(\"--seed\", type=int, default=3, help=\"Training seed.\")\n",
    "parser.add_argument(\n",
    "    \"--save_top_k\",\n",
    "    default=1,\n",
    "    type=int,\n",
    "    help=\"The best k models according to the quantity monitored will be saved.\",\n",
    ")\n",
    "# Early Stopping\n",
    "parser.add_argument(\n",
    "    \"--monitor\", default=\"val_acc\", type=str, help=\"Quantity to monitor.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--metric_mode\",\n",
    "    default=\"max\",\n",
    "    type=str,\n",
    "    help=\"If we want to min/max the monitored quantity.\",\n",
    "    choices=[\"min\", \"max\"],\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--patience\",\n",
    "    default=5,\n",
    "    type=int,\n",
    "    help=(\n",
    "        \"Number of epochs with no improvement \"\n",
    "        \"after which training will be stopped.\"\n",
    "    ),\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--min_epochs\",\n",
    "    default=1,\n",
    "    type=int,\n",
    "    help=\"Limits training to a minimum number of epochs\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--max_epochs\",\n",
    "    default=100,\n",
    "    type=int,\n",
    "    help=\"Limits training to a max number number of epochs\",\n",
    ")\n",
    "\n",
    "# Batching\n",
    "parser.add_argument(\n",
    "    \"--batch_size\", default=1, type=int, help=\"Batch size to be used.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--accumulate_grad_batches\",\n",
    "    default=64,\n",
    "    type=int,\n",
    "    help=(\n",
    "        \"Accumulated gradients runs K small batches of size N before \"\n",
    "        \"doing a backwards pass.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "# gpu/tpu args\n",
    "parser.add_argument(\"--accelerator\", type=str, default=\"auto\", help=\"Which hardware accelerator to use\", choices=[\"cpu\", \"gpu\", \"tpu\"])\n",
    "parser.add_argument(\"--devices\", type=str, default=\"auto\", help=\"How many devices to use\")\n",
    "parser.add_argument(\"--strategy\", type=str, default=\"ddp\", help=\"Which parallelization strategy to use\", choices=[\"dp\", \"ddp\", \"ddp_spawn\", \"ddp2\"])\n",
    "parser.add_argument(\n",
    "    \"--limit_val_batches\",\n",
    "    default=1.0,\n",
    "    type=float,\n",
    "    help=(\n",
    "        \"If you don't want to use the entire validation set (for debugging or \"\n",
    "        \"if it's huge), set how much of the validation set you want to use with this flag.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "# mixed precision\n",
    "parser.add_argument(\"--precision\", type=int, default=\"32\", help=\"full precision or mixed precision mode\")\n",
    "\n",
    "# each LightningModule defines arguments relevant to it\n",
    "parser = ProtTransDisorderPredictor.add_model_specific_args(parser)\n",
    "hparams = parser.parse_known_args()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Main Training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def setup_logger() -> TensorBoardLogger:\n",
    "    \"\"\" Function that sets the TestTubeLogger to be used. \"\"\"\n",
    "    now = datetime.now()\n",
    "    dt_string = now.strftime(\"%d-%m-%Y--%H-%M-%S\")\n",
    "\n",
    "    return TensorBoardLogger(\n",
    "        save_dir=\"logs/\",\n",
    "        version=dt_string,\n",
    "        name=\"lightning_logs\",\n",
    "    )\n",
    "\n",
    "logger = setup_logger()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 3\n",
      "Some weights of the model checkpoint at Rostlab/prot_t5_xl_uniref50 were not used when initializing T5EncoderModel: ['decoder.block.19.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.12.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.2.DenseReluDense.wi.weight', 'decoder.block.16.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.20.layer.0.SelfAttention.k.weight', 'decoder.block.12.layer.2.layer_norm.weight', 'decoder.block.13.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.22.layer.2.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.17.layer.1.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.15.layer.1.EncDecAttention.v.weight', 'decoder.block.22.layer.0.SelfAttention.k.weight', 'lm_head.weight', 'decoder.block.20.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.13.layer.0.layer_norm.weight', 'decoder.final_layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.18.layer.0.SelfAttention.k.weight', 'decoder.block.12.layer.0.SelfAttention.k.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.13.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.2.DenseReluDense.wo.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.12.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.15.layer.2.DenseReluDense.wi.weight', 'decoder.block.19.layer.1.EncDecAttention.k.weight', 'decoder.block.18.layer.2.layer_norm.weight', 'decoder.block.18.layer.1.EncDecAttention.o.weight', 'decoder.block.23.layer.0.layer_norm.weight', 'decoder.block.17.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.1.EncDecAttention.o.weight', 'decoder.block.19.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.0.SelfAttention.v.weight', 'decoder.block.21.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.23.layer.1.EncDecAttention.v.weight', 'decoder.block.17.layer.0.SelfAttention.v.weight', 'decoder.block.22.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.23.layer.0.SelfAttention.o.weight', 'decoder.block.13.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.17.layer.2.DenseReluDense.wi.weight', 'decoder.block.16.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.17.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.20.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.1.EncDecAttention.q.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.12.layer.0.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.0.SelfAttention.q.weight', 'decoder.block.17.layer.1.EncDecAttention.o.weight', 'decoder.block.17.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.13.layer.0.SelfAttention.q.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.18.layer.0.layer_norm.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.23.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.22.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.2.DenseReluDense.wi.weight', 'decoder.block.16.layer.1.EncDecAttention.v.weight', 'decoder.block.21.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.22.layer.2.DenseReluDense.wo.weight', 'decoder.block.23.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.14.layer.1.layer_norm.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.q.weight', 'decoder.block.21.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.2.layer_norm.weight', 'decoder.block.13.layer.1.layer_norm.weight', 'decoder.block.13.layer.1.EncDecAttention.v.weight', 'decoder.block.16.layer.0.SelfAttention.o.weight', 'decoder.block.14.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.2.DenseReluDense.wo.weight', 'decoder.block.18.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.2.DenseReluDense.wi.weight', 'decoder.block.14.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.1.EncDecAttention.q.weight', 'decoder.block.12.layer.1.EncDecAttention.k.weight', 'decoder.block.16.layer.2.DenseReluDense.wi.weight', 'decoder.block.14.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.19.layer.2.DenseReluDense.wi.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.15.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.14.layer.0.SelfAttention.o.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.17.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.v.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.20.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.21.layer.1.EncDecAttention.q.weight', 'decoder.block.15.layer.1.EncDecAttention.k.weight', 'decoder.block.19.layer.1.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.16.layer.2.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.2.layer_norm.weight', 'decoder.block.20.layer.1.layer_norm.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.16.layer.1.EncDecAttention.q.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.16.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.2.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.12.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.19.layer.0.SelfAttention.o.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.14.layer.2.DenseReluDense.wo.weight', 'decoder.block.16.layer.1.layer_norm.weight', 'decoder.block.15.layer.0.SelfAttention.k.weight', 'decoder.block.15.layer.0.SelfAttention.o.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.0.SelfAttention.o.weight', 'decoder.block.22.layer.1.EncDecAttention.q.weight', 'decoder.block.13.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.23.layer.2.layer_norm.weight', 'decoder.block.19.layer.2.DenseReluDense.wo.weight', 'decoder.block.21.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.22.layer.0.layer_norm.weight', 'decoder.block.16.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.22.layer.1.layer_norm.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.19.layer.0.SelfAttention.v.weight', 'decoder.block.23.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.12.layer.2.DenseReluDense.wi.weight', 'decoder.block.18.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.15.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.20.layer.1.EncDecAttention.k.weight', 'decoder.block.21.layer.1.EncDecAttention.k.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.18.layer.2.DenseReluDense.wo.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.15.layer.0.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.13.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.18.layer.1.EncDecAttention.v.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.18.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.17.layer.0.layer_norm.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.o.weight', 'decoder.block.14.layer.0.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.layer_norm.weight', 'decoder.block.21.layer.2.layer_norm.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.22.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.0.SelfAttention.v.weight', 'decoder.block.13.layer.2.layer_norm.weight', 'decoder.block.15.layer.1.layer_norm.weight', 'decoder.block.22.layer.1.EncDecAttention.k.weight', 'decoder.block.21.layer.0.SelfAttention.k.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.15.layer.0.SelfAttention.q.weight', 'decoder.block.23.layer.1.layer_norm.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.23.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.18.layer.0.SelfAttention.o.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.12.layer.1.layer_norm.weight', 'decoder.block.16.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.20.layer.0.layer_norm.weight', 'decoder.block.16.layer.1.EncDecAttention.k.weight', 'decoder.block.19.layer.1.EncDecAttention.v.weight', 'decoder.block.19.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.19.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.v.weight', 'decoder.block.18.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.18.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.embed_tokens.weight', 'decoder.block.21.layer.0.layer_norm.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.12.layer.0.SelfAttention.v.weight', 'decoder.block.16.layer.0.layer_norm.weight', 'decoder.block.18.layer.1.layer_norm.weight', 'decoder.block.21.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.1.EncDecAttention.q.weight', 'decoder.block.15.layer.2.DenseReluDense.wo.weight', 'decoder.block.23.layer.1.EncDecAttention.k.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight']\n",
      "- This IS expected if you are initializing T5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Main training routine specific for this project\n",
    ":param hparams:\n",
    "\"\"\"\n",
    "seed_everything(hparams.seed)\n",
    "\n",
    "model = ProtTransDisorderPredictor(hparams)\n",
    "\n",
    "# Init model checkpoint path and saver\n",
    "ckpt_path = os.path.join(\n",
    "    logger.save_dir,\n",
    "    logger.name,\n",
    "    f\"version_{logger.version}\",\n",
    "    \"checkpoints\",\n",
    ")\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=ckpt_path,\n",
    "    filename=\"{epoch}-{val_loss:.2f}-{val_acc:.2f}\",\n",
    "    save_top_k=hparams.save_top_k,\n",
    "    monitor=hparams.monitor,\n",
    "    every_n_epochs=1,\n",
    "    mode=hparams.metric_mode,\n",
    ")\n",
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=hparams.monitor,\n",
    "    min_delta=0.0,\n",
    "    patience=hparams.patience,\n",
    "    verbose=True,\n",
    "    mode=hparams.metric_mode,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    # fast_dev_run=True,\n",
    "    accelerator=hparams.accelerator,\n",
    "    devices=hparams.devices,\n",
    "    strategy='dp',\n",
    "    logger=logger,\n",
    "    max_epochs=hparams.max_epochs,\n",
    "    min_epochs=hparams.min_epochs,\n",
    "    accumulate_grad_batches=hparams.accumulate_grad_batches,\n",
    "    limit_val_batches=hparams.limit_val_batches,\n",
    "    callbacks=[checkpoint_callback, early_stop_callback],\n",
    "    precision=hparams.precision,\n",
    "    deterministic=False,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gc.collect()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "4305"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "#%reload_ext tensorboard\n",
    "#%tensorboard --logdir f\"{logger.save_dir}{logger.name}/\" --port=8009 --bind_all"
   ],
   "execution_count": 18,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "\n",
      "  | Name         | Type             | Params\n",
      "--------------------------------------------------\n",
      "0 | metric_acc   | Accuracy         | 0     \n",
      "1 | LM           | T5EncoderModel   | 1.2 B \n",
      "2 | lstm         | LSTM             | 8.4 M \n",
      "3 | hidden2label | Linear           | 2.0 K \n",
      "4 | _loss        | CrossEntropyLoss | 0     \n",
      "--------------------------------------------------\n",
      "8.4 M     Trainable params\n",
      "1.2 B     Non-trainable params\n",
      "1.2 B     Total params\n",
      "2,433.081 Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Sanity Checking: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d3d4a66ff7c74bdbb997d6d6a0624805"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "00856097b1e8426cafbdedbdba3967a8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ebio/abt1_share/toolkit_support1/sources/anaconda3/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "data": {
      "text/plain": "Validation: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c67c7d586fe04c80a355002599cca207"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_acc improved. New best score: 0.640\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 124.00 MiB (GPU 0; 10.92 GiB total capacity; 9.87 GiB already allocated; 90.62 MiB free; 10.08 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_906195/4129431813.py\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mtrainer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m/ebio/abt1_share/toolkit_support1/sources/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[1;32m    766\u001B[0m         \"\"\"\n\u001B[1;32m    767\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstrategy\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmodel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 768\u001B[0;31m         self._call_and_handle_interrupt(\n\u001B[0m\u001B[1;32m    769\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_fit_impl\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrain_dataloaders\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mval_dataloaders\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdatamodule\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mckpt_path\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    770\u001B[0m         )\n",
      "\u001B[0;32m/ebio/abt1_share/toolkit_support1/sources/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\u001B[0m in \u001B[0;36m_call_and_handle_interrupt\u001B[0;34m(self, trainer_fn, *args, **kwargs)\u001B[0m\n\u001B[1;32m    719\u001B[0m                 \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstrategy\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlauncher\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlaunch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrainer_fn\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrainer\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    720\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 721\u001B[0;31m                 \u001B[0;32mreturn\u001B[0m \u001B[0mtrainer_fn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    722\u001B[0m         \u001B[0;31m# TODO: treat KeyboardInterrupt as BaseException (delete the code below) in v1.7\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    723\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mKeyboardInterrupt\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mexception\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/ebio/abt1_share/toolkit_support1/sources/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\u001B[0m in \u001B[0;36m_fit_impl\u001B[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[1;32m    807\u001B[0m             \u001B[0mckpt_path\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmodel_provided\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmodel_connected\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlightning_module\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    808\u001B[0m         )\n\u001B[0;32m--> 809\u001B[0;31m         \u001B[0mresults\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_run\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mckpt_path\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mckpt_path\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    810\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    811\u001B[0m         \u001B[0;32massert\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstate\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstopped\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/ebio/abt1_share/toolkit_support1/sources/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\u001B[0m in \u001B[0;36m_run\u001B[0;34m(self, model, ckpt_path)\u001B[0m\n\u001B[1;32m   1232\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_checkpoint_connector\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mresume_end\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1233\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1234\u001B[0;31m         \u001B[0mresults\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_run_stage\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1235\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1236\u001B[0m         \u001B[0mlog\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdetail\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf\"{self.__class__.__name__}: trainer tearing down\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/ebio/abt1_share/toolkit_support1/sources/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\u001B[0m in \u001B[0;36m_run_stage\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1319\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpredicting\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1320\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_run_predict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1321\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_run_train\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1322\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1323\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_pre_training_routine\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/ebio/abt1_share/toolkit_support1/sources/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\u001B[0m in \u001B[0;36m_run_train\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1349\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit_loop\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrainer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1350\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mautograd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mset_detect_anomaly\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_detect_anomaly\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1351\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit_loop\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrun\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1352\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1353\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_run_evaluate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0m_EVALUATE_OUTPUT\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/ebio/abt1_share/toolkit_support1/sources/anaconda3/lib/python3.9/site-packages/pytorch_lightning/loops/base.py\u001B[0m in \u001B[0;36mrun\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    202\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    203\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mon_advance_start\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 204\u001B[0;31m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0madvance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    205\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mon_advance_end\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    206\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_restarting\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mFalse\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/ebio/abt1_share/toolkit_support1/sources/anaconda3/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py\u001B[0m in \u001B[0;36madvance\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    266\u001B[0m         )\n\u001B[1;32m    267\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrainer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprofiler\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprofile\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"run_training_epoch\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 268\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_outputs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mepoch_loop\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrun\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_data_fetcher\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    269\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    270\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mon_advance_end\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/ebio/abt1_share/toolkit_support1/sources/anaconda3/lib/python3.9/site-packages/pytorch_lightning/loops/base.py\u001B[0m in \u001B[0;36mrun\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    202\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    203\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mon_advance_start\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 204\u001B[0;31m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0madvance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    205\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mon_advance_end\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    206\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_restarting\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mFalse\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/ebio/abt1_share/toolkit_support1/sources/anaconda3/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py\u001B[0m in \u001B[0;36madvance\u001B[0;34m(self, data_fetcher)\u001B[0m\n\u001B[1;32m    206\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    207\u001B[0m             \u001B[0;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrainer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprofiler\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprofile\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"run_training_batch\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 208\u001B[0;31m                 \u001B[0mbatch_output\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbatch_loop\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrun\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbatch\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbatch_idx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    209\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    210\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbatch_progress\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mincrement_processed\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/ebio/abt1_share/toolkit_support1/sources/anaconda3/lib/python3.9/site-packages/pytorch_lightning/loops/base.py\u001B[0m in \u001B[0;36mrun\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    202\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    203\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mon_advance_start\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 204\u001B[0;31m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0madvance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    205\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mon_advance_end\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    206\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_restarting\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mFalse\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/ebio/abt1_share/toolkit_support1/sources/anaconda3/lib/python3.9/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py\u001B[0m in \u001B[0;36madvance\u001B[0;34m(self, batch, batch_idx)\u001B[0m\n\u001B[1;32m     86\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrainer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlightning_module\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mautomatic_optimization\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     87\u001B[0m             \u001B[0moptimizers\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_get_active_optimizers\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrainer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moptimizers\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrainer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moptimizer_frequencies\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbatch_idx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 88\u001B[0;31m             \u001B[0moutputs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moptimizer_loop\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrun\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msplit_batch\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moptimizers\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbatch_idx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     89\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     90\u001B[0m             \u001B[0moutputs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmanual_loop\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrun\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msplit_batch\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbatch_idx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/ebio/abt1_share/toolkit_support1/sources/anaconda3/lib/python3.9/site-packages/pytorch_lightning/loops/base.py\u001B[0m in \u001B[0;36mrun\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    202\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    203\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mon_advance_start\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 204\u001B[0;31m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0madvance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    205\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mon_advance_end\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    206\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_restarting\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mFalse\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/ebio/abt1_share/toolkit_support1/sources/anaconda3/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\u001B[0m in \u001B[0;36madvance\u001B[0;34m(self, batch, *args, **kwargs)\u001B[0m\n\u001B[1;32m    201\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    202\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0madvance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbatch\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mAny\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mAny\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mAny\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m  \u001B[0;31m# type: ignore[override]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 203\u001B[0;31m         result = self._run_optimization(\n\u001B[0m\u001B[1;32m    204\u001B[0m             \u001B[0mbatch\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    205\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_batch_idx\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/ebio/abt1_share/toolkit_support1/sources/anaconda3/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\u001B[0m in \u001B[0;36m_run_optimization\u001B[0;34m(self, split_batch, batch_idx, optimizer, opt_idx)\u001B[0m\n\u001B[1;32m    247\u001B[0m             \u001B[0;31m# automatic_optimization=True: perform ddp sync only when performing optimizer_step\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    248\u001B[0m             \u001B[0;32mwith\u001B[0m \u001B[0m_block_parallel_sync_behavior\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrainer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstrategy\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mblock\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 249\u001B[0;31m                 \u001B[0mclosure\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    250\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    251\u001B[0m         \u001B[0;31m# ------------------------------\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/ebio/abt1_share/toolkit_support1/sources/anaconda3/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    146\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    147\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__call__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mAny\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mAny\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mOptional\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mTensor\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 148\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_result\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mclosure\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    149\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_result\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mloss\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    150\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/ebio/abt1_share/toolkit_support1/sources/anaconda3/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\u001B[0m in \u001B[0;36mclosure\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    141\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    142\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_backward_fn\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mstep_output\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mclosure_loss\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 143\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_backward_fn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstep_output\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mclosure_loss\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    144\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    145\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mstep_output\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/ebio/abt1_share/toolkit_support1/sources/anaconda3/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\u001B[0m in \u001B[0;36mbackward_fn\u001B[0;34m(loss)\u001B[0m\n\u001B[1;32m    309\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    310\u001B[0m         \u001B[0;32mdef\u001B[0m \u001B[0mbackward_fn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mloss\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mTensor\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 311\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrainer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_call_strategy_hook\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"backward\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mloss\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moptimizer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mopt_idx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    312\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    313\u001B[0m             \u001B[0;31m# check if model weights are nan\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/ebio/abt1_share/toolkit_support1/sources/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\u001B[0m in \u001B[0;36m_call_strategy_hook\u001B[0;34m(self, hook_name, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1761\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1762\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprofiler\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprofile\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf\"[Strategy]{self.strategy.__class__.__name__}.{hook_name}\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1763\u001B[0;31m             \u001B[0moutput\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1764\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1765\u001B[0m         \u001B[0;31m# restore current_fx when nested context\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/ebio/abt1_share/toolkit_support1/sources/anaconda3/lib/python3.9/site-packages/pytorch_lightning/strategies/strategy.py\u001B[0m in \u001B[0;36mbackward\u001B[0;34m(self, closure_loss, *args, **kwargs)\u001B[0m\n\u001B[1;32m    166\u001B[0m         \u001B[0mclosure_loss\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprecision_plugin\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpre_backward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlightning_module\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mclosure_loss\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    167\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 168\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprecision_plugin\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlightning_module\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mclosure_loss\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    169\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    170\u001B[0m         \u001B[0mclosure_loss\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprecision_plugin\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpost_backward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlightning_module\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mclosure_loss\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/ebio/abt1_share/toolkit_support1/sources/anaconda3/lib/python3.9/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py\u001B[0m in \u001B[0;36mbackward\u001B[0;34m(self, model, closure_loss, optimizer, *args, **kwargs)\u001B[0m\n\u001B[1;32m     78\u001B[0m         \u001B[0;31m# do backward pass\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     79\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mmodel\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpl\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mLightningModule\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 80\u001B[0;31m             \u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mclosure_loss\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moptimizer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     81\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     82\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_run_backward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mclosure_loss\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/ebio/abt1_share/toolkit_support1/sources/anaconda3/lib/python3.9/site-packages/pytorch_lightning/core/lightning.py\u001B[0m in \u001B[0;36mbackward\u001B[0;34m(self, loss, optimizer, optimizer_idx, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1387\u001B[0m                 \u001B[0mloss\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1388\u001B[0m         \"\"\"\n\u001B[0;32m-> 1389\u001B[0;31m         \u001B[0mloss\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1390\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1391\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mtoggle_optimizer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moptimizer\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mUnion\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mOptimizer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mLightningOptimizer\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moptimizer_idx\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mint\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/ebio/abt1_share/toolkit_support1/sources/anaconda3/lib/python3.9/site-packages/torch/_tensor.py\u001B[0m in \u001B[0;36mbackward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    361\u001B[0m                 \u001B[0mcreate_graph\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcreate_graph\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    362\u001B[0m                 inputs=inputs)\n\u001B[0;32m--> 363\u001B[0;31m         \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mautograd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgradient\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    364\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    365\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mregister_hook\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mhook\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/ebio/abt1_share/toolkit_support1/sources/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py\u001B[0m in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    171\u001B[0m     \u001B[0;31m# some Python versions print out the first line of a multi-line function\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    172\u001B[0m     \u001B[0;31m# calls in the traceback and some print out the last line\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 173\u001B[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001B[0m\u001B[1;32m    174\u001B[0m         \u001B[0mtensors\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgrad_tensors_\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    175\u001B[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001B[0;32m/ebio/abt1_share/toolkit_support1/sources/anaconda3/lib/python3.9/site-packages/torch/autograd/function.py\u001B[0m in \u001B[0;36mapply\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m    251\u001B[0m                                \"of them.\")\n\u001B[1;32m    252\u001B[0m         \u001B[0muser_fn\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mvjp_fn\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mvjp_fn\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mFunction\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvjp\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0mbackward_fn\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 253\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0muser_fn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    254\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    255\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mapply_jvp\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/ebio/abt1_share/toolkit_support1/sources/anaconda3/lib/python3.9/site-packages/torch/utils/checkpoint.py\u001B[0m in \u001B[0;36mbackward\u001B[0;34m(ctx, *args)\u001B[0m\n\u001B[1;32m    144\u001B[0m                 \u001B[0;34m\"none of output has requires_grad=True,\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    145\u001B[0m                 \" this checkpoint() is not necessary\")\n\u001B[0;32m--> 146\u001B[0;31m         \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mautograd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moutputs_with_grad\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0margs_with_grad\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    147\u001B[0m         grads = tuple(inp.grad if isinstance(inp, torch.Tensor) else None\n\u001B[1;32m    148\u001B[0m                       for inp in detached_inputs)\n",
      "\u001B[0;32m/ebio/abt1_share/toolkit_support1/sources/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py\u001B[0m in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    171\u001B[0m     \u001B[0;31m# some Python versions print out the first line of a multi-line function\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    172\u001B[0m     \u001B[0;31m# calls in the traceback and some print out the last line\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 173\u001B[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001B[0m\u001B[1;32m    174\u001B[0m         \u001B[0mtensors\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgrad_tensors_\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    175\u001B[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001B[0;31mRuntimeError\u001B[0m: CUDA out of memory. Tried to allocate 124.00 MiB (GPU 0; 10.92 GiB total capacity; 9.87 GiB already allocated; 90.62 MiB free; 10.08 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "best_checkpoint_path = glob.glob(ckpt_path + \"/*\")[0]\n",
    "print(best_checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "trainer.resume_from_checkpoint = best_checkpoint_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_901433/2699859066.py\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mbest_checkpoint_path\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mglob\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mglob\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf\"{ckpt_path}/*\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbest_checkpoint_path\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mIndexError\u001B[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "best_checkpoint_path = glob.glob(f\"{ckpt_path}/*\")[0]\n",
    "print(best_checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Predict new sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "seq = \"MSDNDDIEVESDEEQPRFQSAADKRAHHNALERKRRDHIKDSFHSLRDSVPSLQGEKASRAQILDKATEYIQYMRRKNHTHQQDIDDLKRQNALLEQQVRALEKARSSAQLQTNYPSSDNSLYTNAKGSTISAFDGGSDSSSESEPEEPQSRKKLRMEAS\"\n",
    "label = \"0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000011111111111111111111111111111111111111111111111111111111110000000000\"\n",
    "\n",
    "preds = model.predict({\"seq\": seq})\n",
    "\n",
    "print(\"Sequence label is: {} - prediction is: {}\".format(label, preds['predicted_label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "seq = \"MSDNDDIEVESDEEQPRFQSAADKRAHHNALERKRRDHIKDSFHSLRDSVPSLQGEKASRAQILDKATEYIQYMRRKNHTHQQDIDDLKRQNALLEQQVRALEKARSSAQLQTNYPSSDNSLYTNAKGSTISAFDGGSDSSSESEPEEPQSRKKLRMEAS\"\n",
    "label = \"0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000011111111111111111111111111111111111111111111111111111111110000000000\"\n",
    "\n",
    "predictions = model.predict({\"seq\": seq})\n",
    "\n",
    "print(\"Sequence label is: {} - prediction is: {}\".format(label, predictions['predicted_label']))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Hyperparameter Tuning\n",
    "Using RayTune, I tested different models (ProtT5-XL-encoder, ProtBert, ESM-1b) with different parameters in two rounds (results in folder 'raytune_results' on raven, first 'tune_esm_prottrans_' then 'tune_rates').\n",
    "\n",
    "The best model I found was: `{'model_name': 'Rostlab/prot_t5_xl_half_uniref50-enc', 'rnn': 'gru', 'crf_after_rnn': False, 'learning_rate': 2e-04, 'encoder_learning_rate': 1e-05, 'hidden_features': 1024}`\n",
    "\n",
    "We still need to do timing tests, and I could imagine an ESM-1b model without dropout linear net to also work well while being faster. Test this!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}