{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Predicting continuous disorder\n",
    "\n",
    "Try dataset from [SETH](https://www.biorxiv.org/content/10.1101/2022.06.23.497276v1.full.pdf) with different architectures."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data\n",
    "\n",
    "We contacted Rostlab, and they released their 1174 entry training dataset. As validation set, we use 15% of that i.e. 176 sequences.\n",
    "One issue is that they have an ignore index (999) but PyTorch's MSELoss does not support that currently. Therefore, we built this ourselves."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### First Results\n",
    "\n",
    "The performance for all models was way below the papers reported scores (around 0.55 spearman and 0.7 AUROC). This could be due to the following reasons:\n",
    "\n",
    "- The data distributions of train and val sets are too dissimilar => New random sets (`chezod_cnn_random_shuffle_data.out`). Result: Improved it to 0.64 spearman and 0.86 AUROC\n",
    "- Half-precision could be at fault => Try full precision (`chezod_cnn_full_precision.out`). Result: Did not improve it\n",
    "- Combine: Full precision and random data without DP (`chezod_cnn_random_full_precision.out`). Result: Same as with random data but half-precision\n",
    "- Different metric computation => Run their SETH implementation over our val set. Result: Indeed, we were calculating it incorrectly. The padded values (999) should be excluded and for AUROC, a perfect match should not count as 0 but as 1.\n",
    "\n",
    "Running SETH on Colab over our validation sets gave the following metrics:\n",
    "\n",
    "- Val Set 1\n",
    "   - Mean Masked AUROC: 0.876\n",
    "   - Mean Masked Spearman: 0.558\n",
    "- Val Set 2\n",
    "   - Mean Masked AUROC: 0.902\n",
    "   - Mean Masked Spearman: 0.7\n",
    "\n",
    "For our CNN training, we got:\n",
    "\n",
    "- Val Set 1 (`chezod_cnn_corrected_scoring_set1.out`)\n",
    "   - Mean Masked AUROC: 0.87\n",
    "   - Mean Masked Spearman: 0.64\n",
    "- Val Set 2 (`chezod_cnn_corrected_scoring_set2.out`)\n",
    "   - Mean Masked AUROC: 0.90\n",
    "   - Mean Masked Spearman: 0.79"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Hyperparameter Optimization\n",
    "\n",
    "Running 30 rounds to select best embedding, architecture (cnn, rnn, linear), and learning rates (`chezod_hyperparam.out`). First, we failed because the maximum length calculation was incorrect. Here are the hyperparameters:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"model_name\": tune.choice(['facebook/esm-1b', 'Rostlab/prot_bert_bfd', 'Rostlab/prot_t5_xl_half_uniref50-enc']),\n",
    "    \"architecture\": tune.choice(['rnn', 'cnn', 'linear']),\n",
    "    \"learning_rate\": tune.loguniform(1e-5, 1e-2),\n",
    "    \"encoder_learning_rate\": tune.loguniform(5e-6, 1e-2),\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
